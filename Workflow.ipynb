{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'HPC-FAIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Set grammar graph...     \n",
      "\n",
      "-- Log: Reading grammar file and pre-processing text...     \n",
      "\n",
      "-- Log: Grammar text preparation finished     \n",
      "\n",
      "-- Log: Start building grammar tree...     \n",
      "\n",
      "-- Log: Add API description...     \n",
      "\n",
      "-- Log: Set nodes max formal children length     \n",
      "\n",
      "-- Log: Grammar tree build finished.     \n",
      "\n",
      "-- Log: Grammar graph built     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build grammar graph\n",
    "import HISyn.domain_knowledge.DomainKnowledgeConstructor as dkc\n",
    "gg = dkc.set_grammar_graph(domain, './Documentation/' + domain + '/grammar-gen.txt', './Documentation/' + domain + '/API_documents.txt', reload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BuildingDataset ['_output_log'] {'_kernel_data_best_csv', '_kernel_stat_csv'} Merge kernel-level and data-level features by outer-join norm         \n",
      "ModelTraining ['_GPUTrace_csv', '_dataset_csv'] {'_mergedDataSet_csv'} collect & sort time measurements from all variants and report the top varaiant for each each kernel and data input norm         \n",
      "ModelEvaluation ['_kernel_data_best_csv', '_mergedDataSet_csv'] {'_labelledData_csv'} Adding the label of best-performing variants to the merged data norm         \n",
      "DataPreProcessing ['_GPUTraceOutput_log'] {'_GPUTrace_csv'} collect & filter baseline data-level features norm         \n",
      "DataCollection ['_nsight_log'] {'_dataset_csv'} collect baseline kernel-level features norm         \n",
      "_root [['_kernel_stat_csv'], ['_labelledData_csv']] set() ['_kernel_stat_csv', '_labelledData_csv']          \n",
      "_kernel_data_best_csv [['BuildingDataset']] {'ModelEvaluation'} ['BuildingDataset(_output_log)']          \n",
      "_kernel_stat_csv [['BuildingDataset']] {'_root'} ['BuildingDataset(_output_log)']          \n",
      "_mergedDataSet_csv [['ModelTraining']] {'ModelEvaluation'} ['ModelTraining(_GPUTrace_csv,_dataset_csv)']          \n",
      "_labelledData_csv [['ModelEvaluation']] {'_root'} ['ModelEvaluation(_kernel_data_best_csv,_mergedDataSet_csv)']          \n",
      "_GPUTrace_csv [['DataPreProcessing']] {'ModelTraining'} ['DataPreProcessing(_GPUTraceOutput_log)']          \n",
      "_dataset_csv [['DataCollection']] {'ModelTraining'} ['DataCollection(_nsight_log)']          \n"
     ]
    }
   ],
   "source": [
    "gg.build_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NL query\n",
    "text = 'find the labelled data'\n",
    "\n",
    "# get query from test cases\n",
    "import HISyn.front_end.front_end_function_kit as front_kit\n",
    "# index = 0\n",
    "# text = front_kit.read_text('./Documentation/' + domain + '/text_new.txt', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CORENLP_HOME=../HISyn/third_party_pkgs/stanford-corenlp-full-2018-10-05\n",
      "-- Log: Parsing text...     \n",
      "\n",
      "-- Log: Check NLP server status...     \n",
      "\n",
      "-- Log: NLP server already started!     \n",
      "\n",
      "test:  find the labelled data            \n",
      "-- Log: Start parsing sentence: find the labelled data     \n",
      "\n",
      "-- Log: starting JAVA Stanford CoreNLP Server...     \n",
      "\n",
      "-- Log: Parsing finished     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parsing the query and prune the unimportant edges.\n",
    "# NLP\n",
    "get_ipython().run_line_magic('env', 'CORENLP_HOME=../HISyn/third_party_pkgs/stanford-corenlp-full-2018-10-05')\n",
    "nlp = front_kit.nlp_parsing(text, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: apply domain_specific_parsing_rules...     \n",
      "\n",
      "-- Log: no domain specific rules.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "front_kit.domain_specfic_parsing_rules(domain, gg, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Pruning unimportant edges...     \n",
      "\n",
      "-- Log: Pruning unimportant edges...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import HISyn.common_knowledge.NLPCommonKnowledge as nlpck\n",
    "front_kit.prune_edges(nlp, nlpck.prunable_dep_tags, nlpck.prunable_pos_tags, nlpck.common_knowledge_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "find [VB] [O] [find] [] --dobj--> data [NNS] [O] [datum] [] ====>> [] []\n",
      "             \n",
      "data [NNS] [O] [datum] [] --amod--> labelled [VBN] [O] [label] [] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HISyn.back_end.back_end_function_kit as back_kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: checking common knowledge     \n",
      "\n",
      "-- Log: common knowledge replace finished     \n",
      "\n",
      "-- Log: start keywords mapping     \n",
      "\n",
      "find ['merge', 'kernel-level', 'and', 'data-level', 'features', 'by', 'outer-join'] False\n",
      "find ['collect', '&', 'sort', 'time', 'measurements', 'from', 'all', 'variants', 'and', 'report', 'the', 'top', 'varaiant', 'for', 'each', 'each', 'kernel', 'and', 'data', 'input'] False\n",
      "find ['adding', 'the', 'label', 'of', 'best-performing', 'variants', 'to', 'the', 'merged', 'data'] False\n",
      "find ['collect', '&', 'filter', 'baseline', 'data-level', 'features'] False\n",
      "find ['collect', 'baseline', 'kernel-level', 'features'] False\n",
      "find ['merge', 'kernel-level', 'and', 'data-level', 'features', 'by', 'outer-join'] False\n",
      "find ['collect', '&', 'sort', 'time', 'measurements', 'from', 'all', 'variants', 'and', 'report', 'the', 'top', 'varaiant', 'for', 'each', 'each', 'kernel', 'and', 'data', 'input'] False\n",
      "find ['adding', 'the', 'label', 'of', 'best-performing', 'variants', 'to', 'the', 'merged', 'data'] False\n",
      "find ['collect', '&', 'filter', 'baseline', 'data-level', 'features'] False\n",
      "find ['collect', 'baseline', 'kernel-level', 'features'] False\n",
      "data ['merge', 'kernel-level', 'and', 'data-level', 'features', 'by', 'outer-join'] False\n",
      "data ['collect', '&', 'sort', 'time', 'measurements', 'from', 'all', 'variants', 'and', 'report', 'the', 'top', 'varaiant', 'for', 'each', 'each', 'kernel', 'and', 'data', 'input'] True\n",
      "data ['adding', 'the', 'label', 'of', 'best-performing', 'variants', 'to', 'the', 'merged', 'data'] True\n",
      "data ['collect', '&', 'filter', 'baseline', 'data-level', 'features'] False\n",
      "data ['collect', 'baseline', 'kernel-level', 'features'] False\n",
      "labelled ['merge', 'kernel-level', 'and', 'data-level', 'features', 'by', 'outer-join'] False\n",
      "labelled ['collect', '&', 'sort', 'time', 'measurements', 'from', 'all', 'variants', 'and', 'report', 'the', 'top', 'varaiant', 'for', 'each', 'each', 'kernel', 'and', 'data', 'input'] False\n",
      "labelled ['adding', 'the', 'label', 'of', 'best-performing', 'variants', 'to', 'the', 'merged', 'data'] True\n",
      "labelled ['collect', '&', 'filter', 'baseline', 'data-level', 'features'] False\n",
      "labelled ['collect', 'baseline', 'kernel-level', 'features'] False\n",
      "-- Log: keywords mapping finished     \n",
      "\n",
      "-- Log: apply domain_specific mapping rules...     \n",
      "\n",
      "-- Log: no domain specific rules.     \n",
      "\n",
      "-- Log: remove empty edges     \n",
      "\n",
      "-- Log: empty edge removed.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "back_kit.semantic_mapping(domain, gg, nlp, nlpck.common_knowledge_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "find [VB] [O] [find] [] --dobj--> data [NNS] [O] [datum] ['ModelTraining', 'ModelEvaluation'] ====>> [] []\n",
      "             \n",
      "data [NNS] [O] [datum] ['ModelTraining', 'ModelEvaluation'] --amod--> labelled [VBN] [O] [label] ['ModelEvaluation'] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: start longest matching     \n",
      "\n",
      "-- Log: remove empty edges     \n",
      "\n",
      "-- Log: empty edge removed.     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "back_kit.longest_matching(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "find [VB] [O] [find] [] --dobj--> data [NNS] [O] [datum] ['ModelEvaluation'] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: set preposition mappings ...     \n",
      "\n",
      "-- Log: subj reordering ...     \n",
      "\n",
      "-- Log: checking no governor dependency source     \n",
      "\n",
      "-- Log: add root token to no governor source     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "[dependent_dict, no_gov_source_dict, empty_mapping_source_edge_dict, root_index] = back_kit.reordering(nlp, gg, nlpck.preposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "root [] [] [] [] --dobj--> data [NNS] [O] [datum] ['ModelEvaluation'] ====>> [] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: Starting all paths search ...     \n",
      "\n",
      "-- Log: singel start BFS...     \n",
      "\n",
      "-- Log: Start API:  ModelEvaluation    \n",
      "\n",
      "-- Log: endAPIset:  []    \n",
      "\n",
      "-- Log: path_limit:  None    \n",
      "\n",
      "-- Log: total paths:  1    \n",
      "\n",
      "-- Log: Edges reordering...      \n",
      "\n",
      "-- Log: replace common knowledge api...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "back_kit.reversed_all_path_searching(domain, nlp, gg, dependent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dependency graph -----------------             \n",
      "root [] [] [] [] --dobj--> data [NNS] [O] [datum] ['ModelEvaluation'] ====>> [['ModelEvaluation']] []\n",
      "             \n",
      "--------------------------------------------------             \n"
     ]
    }
   ],
   "source": [
    "nlp.displayByEdge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: find siblings on dependency graph...     \n",
      "\n",
      "-- Log: attach_non_sibling_edges...     \n",
      "\n",
      "-- Log: Path selection: min path/prefix tree heuristic     \n",
      "\n",
      "-- Log: connect all cgts whose root is not root     \n",
      "\n",
      "-- Log: connect cgts to root cgts     \n",
      "\n",
      "-- Log: Set all cgt combinations ...     \n",
      "\n",
      "-- Log: Combine cgts in each group ...     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_cgt_list = back_kit.path_selection_and_combination(nlp, gg, dependent_dict, root_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| root |  |         \n",
      "--| _root |  |         \n",
      "----| _labelledData_csv |  |         \n",
      "------| ModelEvaluation |  |         \n"
     ]
    }
   ],
   "source": [
    "for t in final_cgt_list:\n",
    "    t.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Log: checking grammar on cgts...     \n",
      "\n",
      "-- Log: changing argument order of valid nodes     \n",
      "\n",
      "-- Log: removing trees with wrong grammar...     \n",
      "\n",
      "-- Log: adding default argument to complete the cgt...     \n",
      "\n",
      "-- Log: checking grammar on cgts...     \n",
      "\n",
      "-- Log: changing argument order of valid nodes     \n",
      "\n",
      "-- Log: converting to codes...     \n",
      "\n",
      "-- Log: converting to codes...     \n",
      "\n",
      "-- Log: promising exprs:  ['ModelEvaluation(BuildingDataset(_output_log),ModelTraining(DataPreProcessing(_GPUTraceOutput_log),DataCollection(_nsight_log)))']    \n",
      "\n",
      "-- Log: full candidate list:  1    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "[min_expr, expr_list] = back_kit.code_generation(gg, final_cgt_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
