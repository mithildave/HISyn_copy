#!/usr/bin/env python
# coding: utf-8

# In[1]:


from stanfordnlp.server import CoreNLPClient
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import *


stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()


# In[2]:


# conceptList = [
#     'hpc:Cpu', 'hpc:Memory', 'hpc:Gpu']
#
# propertylist = [
#     'hpc:wgSize','hpc:localMem','hpc:coprocessorSpeed', 'hpc:cpuFrequency',
#     'hpc:modelType', 'hpc:modelFormat', 'hpc:version', 'hpc:learningType',
#     'hpc:subject', 'hpc:license', 'hpc:name', 'hpc:fundedBy', 'hpc:programArea',
#     'hpc:memberOf', 'hpc:affiliation', 'hpc:jobTitle',
#     'hpc:contributor', 'hpc:generatedBy', 'hpc:usedBy']

phrase_pos_tag = ['NN', 'NNP', 'NNS', 'NP', 'WHNP', 'VB','VBP','VBZ','VBD','VBN','VBG']


# In[3]:


property_dict = {
    'hpc:modelType':'domains, type', 'hpc:modelFormat':' script format', 'hpc:version':'version number', 'hpc:learningType':'Learning type',
    'hpc:subject':'subjects or topics', 'hpc:license':'license number', 'hpc:name':' name ', 'hpc:fundedBy':'Organization funded by', 'hpc:programArea':'program area',
    'hpc:memberOf':'member of organization', 'hpc:affiliation':'affiliation', 'hpc:jobTitle':'job position', 'hpc:targetMachine':'target machine',
    'hpc:contributor':' contributor, author, coauthor, create', 'hpc:generatedBy':'generated by', 'hpc:usedBy':'used by'}

concept_dict = {
    'hpc:Cpu': 'central processing unit cpu', 'hpc:Memory':'memory or storage',
    'hpc:Gpu':'graphics processing unit gpu', 'hpc:cpuFrequency':'Frequency of CPUs',
    'hpc:wgSize':'memory size','hpc:localMem':'local memory per thread ','hpc:coprocessorSpeed':'Frequency of coprocessors'
}

class_dict = {
    'hpc:experiment':'experiment',
}


# In[4]:

# In[5]:


# log.log('starting JAVA Stanford CoreNLP Server...')
# setting nlp configurations
# %env CORENLP_HOME=/Users/zifannan/Documents/code/java/stanford-corenlp-full-2018-10-05




# In[6]:


class ParseTree:
    def __init__(self, root):
        self.root = root
        self.np_list = []
        self.all_comb = []
        
    def display(self, level = 0):
        self.root.display()
        
        
class ParseTreeNode:
    def __init__(self, pos = None, phrase = None, parent = None):
        self.pos = pos
        self.phrase = phrase
        self.child = []
        self.ranked_tag = []
        self.parent = parent
        
    def display(self, level = 0):
        print('--'*level, '|',self.pos, ' | ', self.phrase)
        if self.child:
            for c in self.child:
                c.display(level = level + 1)


# In[7]:


def create_parse_tree(pt, parse_tree_node, word_list):
    if pt.child:
        for pt_c in pt.child:
            if pt_c.value in word_list:
                tmp = ParseTreeNode(phrase = pt_c.value, parent = parse_tree_node)
            else:
                tmp = ParseTreeNode(pos = pt_c.value, parent = parse_tree_node)
            parse_tree_node.child.append(tmp)
            create_parse_tree(pt_c, tmp, word_list)




# In[9]:


def relocate_prop(node):
    remove_list = []
    for c in node.child:
        if c.pos in ['IN']:
            if node.parent.pos in ['VP']:
                for pc in node.parent.child:
                    if 'V' in pc.pos:
                        pc.child.append(c)
                        remove_list.append(c)

        relocate_prop(c)
        
    for i in remove_list:
#         i.display()
#         print('*'*20)
#         node.display()
        if i in node.child:
            node.child.remove(i)
        



# In[10]:


def get_phrase(parse_tree_node, parse_tree):
    if parse_tree_node.child:
        tmp = []
        for c in parse_tree_node.child:
            tmp.append(get_phrase(c, parse_tree))
        parse_tree_node.phrase = ' '.join(tmp)
        if parse_tree_node.pos in phrase_pos_tag:
            parse_tree.np_list.append(parse_tree_node)
        if parse_tree_node.pos in ['WP$', 'DT']:
            print('node:', parse_tree_node.phrase)
            return ('')

        return (parse_tree_node.phrase)
    else:
        return (parse_tree_node.phrase)


def keyword_mapping(phrase, sentence):
    keyword_list = phrase.split(' ')
    compare_list = sentence.replace(',', ' ').split(' ')
    
    # preprocess words in keyword_list and compare_list
    for i, word in enumerate(keyword_list):

        keyword_list[i] = stemmer.stem(lemmatizer.lemmatize(word.lower()))
        # print(keyword_list[i])
    for i, cmpr in enumerate(compare_list):
        compare_list[i] = stemmer.stem(lemmatizer.lemmatize(cmpr.lower()))
    
#     print(f'phrase: {keyword_list}\nsentence: {compare_list}')
    
    # match
    for k in keyword_list:
        if k not in compare_list:
            return False
    return True
        


# In[15]:

def mapping(parse_tree):
    for node in parse_tree.np_list:
        print(node.pos, '|', node.phrase)

        for key in property_dict:
            if keyword_mapping(node.phrase, property_dict[key]):
                node.ranked_tag.append([key, property_dict[key]])

        for key in concept_dict:
            if keyword_mapping(node.phrase, concept_dict[key]):
                node.ranked_tag.append(['schema:domainIncludes ' + key, concept_dict[key]])

        for key in class_dict:
            if keyword_mapping(node.phrase, class_dict[key]):
                node.ranked_tag.append([key, class_dict[key]])

    for node in parse_tree.np_list:
        print('--'*10, node.phrase, '--'*10 + '\n', node.ranked_tag)


# In[16]:

def create_comb(parse_tree):
    replace_list = []
    for node in parse_tree.np_list:
        tmp = []
        if node.ranked_tag:
            for tag in node.ranked_tag:
                tmp.append([node, tag[0]])
        else:
            continue
        replace_list.append(tmp)

    # print(replace_list, len(replace_list))

    # create all permutaion of the replace tags

    import itertools

    parse_tree.all_comb = list(itertools.product(*replace_list))
    print(parse_tree.all_comb)


# In[17]:


def restore_prop(node):
    # print('restoring prepositions...')
    #
    # node.display()
    # print('*' * 20)

    remove_list = []
    for c in node.child:
        if c.pos in ['IN']:
            if node.parent.pos in ['VP']:
                for pc in node.parent.child:
                    if pc.pos in ['PP']:
                        c.pos += '-RES'
                        pc.child.insert(0, c)
                        # pc.display()
                        # print('*' * 40)
                        remove_list.append(c)

        restore_prop(c)

    for i in remove_list:
        if i in node.child:
            node.child.remove(i)


def generate_formatted_query(node):
    if node.child:
        tmp = []
        category = None
        prop_flag = False
        cat = None
        for c in node.child:
            if c.pos in ['hpc']:
                if c.phrase in class_dict:
                    phrase = '<' + c.phrase + '>'
                    cat = 'class'
                elif c.phrase in property_dict:
                    phrase = '<' + c.phrase
                    cat = 'property'
                elif c.phrase.split(' ')[1] in concept_dict:
                    phrase = '<' + c.phrase + '>'
                    cat = 'concept'
                else:
                    print('error tag:', c.phrase)
            elif c.pos in ['DT']:
                continue
            else:
                phrase, cat = generate_formatted_query(c)
                
            
            if cat == 'property':
                prop_flag = True
            tmp.append(phrase)
        
        node.phrase = ' '.join(tmp)
        if prop_flag:
            node.phrase += '>'
        return node.phrase, category
    else:
        return node.phrase, None


def create_formatted_query(parse_tree):
    formatted_query_list = []
    parse_tree.display()
    print('-' * 40)

    for i, comb in enumerate(parse_tree.all_comb):

        for replcae in comb:
            print("replace",replcae)
            # replace phrase with tag
            replcae[0].phrase = replcae[1]
            replcae[0].pos = 'hpc'
        parse_tree.display()

        # generate sentence
        formatted_query_list.append(generate_formatted_query(parse_tree.root))

    # print(formatted_query_list)
    return formatted_query_list




def DSL_ontology_split(query):
    annotator_list = ['tokenize', 'ssplit', 'pos', 'lemma', 'ner', 'parse', 'depparse']
    client = CoreNLPClient(start_server=False, annotators=annotator_list, timeout=60000, memory='8G')
    sentence = client.annotate(query).sentence[0]

    word_list = [t.word for t in sentence.token]
    pt = sentence.parseTree
    # In[8]:

    parse_tree = ParseTree(ParseTreeNode(pos=pt.value))
    parse_tree.root.parent = parse_tree.root

    create_parse_tree(pt, parse_tree.root, word_list)

    parse_tree.display()

    relocate_prop(parse_tree.root)

    parse_tree.display()

    get_phrase(parse_tree.root, parse_tree)
    parse_tree.display()

    for phrase in parse_tree.np_list:
        print(phrase.pos, '|', phrase.phrase)

    # In[13]:

    for node in parse_tree.np_list:
        print(node.pos, '|', node.phrase)

    # In[14]:
    mapping(parse_tree)

    restore_prop(parse_tree.root)
    create_comb(parse_tree)
    formatted_query_list = create_formatted_query(parse_tree)

    for query in formatted_query_list:
        query = query[0]
        query = query.replace(" '","\"").replace('` ','"')
        print(query)

    print(formatted_query_list)



if __name__ == '__main__':
    query1 = 'Get CPU related columns from dataset with subject "Y"'
    query2 = 'get project whose program area is "OpenMP" '
    query3 = 'get project funded by X'
    query5 = 'Get the hardware with version Y and used by the experiment with name X'
    query6 = 'Get the hardware used by the experiment with name X'
    query7 = 'list AI model with format "python notebook"'

    query8 = 'get all columns from dataset named "X"'
    query9 = 'get cpu related columns from dataset named "X"'
    query10 = 'get normalization results of columns "X,Y" from file with name "Z"'
    query11 = 'get all contributors for project which is named "GPU Kernel"'
    query12 = 'get all files having contributors named "Lee"'
    query13 = 'files having contributor with name "Lee"'
    query14 = 'extract data related cpu from file "X" '
    query15 = 'get the data deal with cpu frequency and come from file "Y"'
    query16 = 'datasets which were contributed by a person named "Lee"'


    q0 = "get ai model with model format 'python'"
    q1 = "Get memory related data from file 'cgo-nvidiacsv' "
    q2 = "get AI model that has model type 'prediction'"
    q3 = "get AI model having model type 'classification'"
    q4 = "list of  AI models with format 'python notebook'"
    q5 = "get AI model with version '1.2.1' and learning type 'Supervised'"
    q6 = "get AI model having learning type 'Supervised' and subject 'gradient decent'"
    q7 = "Get dataset with subject 'GPGPU'"
    q8 = "get dataset having subject 'Heterogeneous System'"
    q9 = "return dataset whose license is 'GPL-3.0'"
    q10 = "get dataset whose version is '0.2.0' and subject 'LLVM'"
    q11 = "get dataset with name 'dataflow_logs_20.06.01.tar.bz2'"
    q12 = "get project funded by 'Lawrence Livermore National Laboratory'"
    q13 = "get project in program area 'OpenMP'"
    q14 = "get project that is funded by 'ICT' and having program area of 'parallel computing'"
    q15 = "get person who is a member of 'Center for Applied Scientific Computing'"
    q16 = "get person having affiliation 'Lawrence Livermore National Laboratory, CA'"
    q17 = "get person having job title of 'Senior Computer Scientist' and gender 'male'"

    q18 = "get all columns from dataset named 'X'"
    q19 = "get cpu related columns from dataset named 'X'"
    q20 = "get normalization results of columns 'X,Y' from file with name 'Z'"
    q21 = "get all files having contributors with name 'Lee'"
    q22 = "get dataset generated by experiment named 'Optimal Unified Memory - Online Inference'"
    q23 = "get software having version '16.2'  and is used by project named 'X'"
    q24 = "get hardware used by experiment named 'OPUM'"
    q25 = "merge dataset 'X' and dataset having license 'Apache'"
    q26 = "convert file having contributor named 'Caleb' to JSON"

    q27 = "list of  AI models with format 'python notebook'"
    q28 = "return all data columns from file 'X' except 'P'"
    q29 = "give data dealing with cpu freqency from file 'Y'"
    q30 = 'extract data linked to Cpu from file "X"'
    q31 = "normalize the columns 'X,Y' from file named 'Z'"
    q32 = "get a list of people contributed to a project named 'GPU kernal'"
    q33 = "get files having contributor with name 'Lee'"
    q34 = "get files which are created by 'Lee'"
    q35 = "get datasets which were contributed by a person named 'Lee'"
    q36 = "get software having version '16.2'  and is used by project named 'X'"
    q37 = "get hardware which was used during experiment named 'OPUM'"
    q38 = "hardware that was used by experiment named 'OPUM'"
    q39 = "merge dataset 'X' with another dataset having license 'Apache'"
    q40 = "merge dataset 'X' with another dataset license 'Apache'"
    q41 = "convert file having contributor named 'caleb' to json format"

    q42 ="Get the multiplication results of columns X and Y from dataset Z"
    q43 ="For input dataset1.csv, select columns representing performance metrics named “GPU cycles” and “memory utilization rates” about kernel named “function 1”, store the results into dataset1-filtered.csv"
    q44 ="Select: benchmark_name, gpu_type, execution_time from any dataset where:“Hardware” contains “GPU” “Software” contains “Benchmark”"
    q45 ="return all the column names of this dataset."
    q46 ="Get the dataset X from dataset Y without column 'Memory Frequency'."
    q47 ="Select X1,X2 from dataset A and Y1, Y2 from dataset B then merge them into dataset C"
    q48 ="Get the normalized results for column X from dataset A"
    q49 ="Get the encoding results for column X from dataset A"
    q50 ="Convert data file X to CSV format type"
    q51 ="Convert data file Y to JSON format type"
    q52 ="Get the merging results of columns X,Y,Z from dataset W"
    q53 ="Get count of column X from dataset A Group By Model"
    q54 ="Get count of column X from dataset A Group By ‘Model’ and Bench Number"
    q55 ="merge dataset X with dataset Y"
    q56 ="get encoding result for column X from file A with type one hot encoding"
    q57 ="Get all columns from dataset Y without column ‘Memory Frequency’"
    q58 ="Get all columns from dataset Y"

    q60 = "Get memory related data from file 'cgo-nvidia.csv'."
    q61 = "get AI model having model type 'prediction' and target machine named 'lassen'"

    query = q60


    DSL_ontology_split(query)



